\chapter{ Core principles of functional programming in Scala for data engineering }
%\label{chap:functional_programming}
%\addcontentsline{toc}{chapter}{\numberline{\thechapter}The Functional Programming Paradigm}

This chapter introduces the Scala programming language, and related concepts. The chapter begins with an explanation of what makes Scala a great and scalable tool for data engineering and how it can help solve problems efficiently. Then the chapter delves deeper, explaining introduced concepts.

\section{Background}

Scala stands for \textbf{“scalable language”}. Scala was created by \textbf{Martin Odersky}, a German computer scientist and professor at École Polytechnique Fédérale de Lausanne (EPFL) in Switzerland. Odersky had formerly worked on the Java compiler and generics at Sun Microsystems. His vision was to develop a language that seamlessly combined the strengths of both object-oriented and functional programming paradigms. He started working on Scala in 2001 at EPFL His goal was to create an expressive typed language that could run on the \textbf{Java Virtual Machine (JVM)}. The first official version of Scala was released in 2003, and since then, it has gained popularity, evolving into a stable language with an extensive array of libraries and tools.\footnotemark \footnotetext{\fullcite{oderskyOverviewScalaProgramming2006}}

Scala is a programming language that merges ideas from object-oriented and functional programming realms. Its design is influenced by several existing languages. Scala inherits its syntax and object-oriented features from Java, making it easy for Java developers to learn and allowing seamless interoperability with Java code. At the same time, Scala incorporates many functional programming concepts from languages like Haskell and ML such as — immutable data structures, higher-order functions, and pattern matching —. Scala's type system draws inspiration from the ML family of languages, offering a static type system with type interface. Scala also strives to be concise and expressive, similar to scripting languages like Python. It includes features such, as operator overloading and implicit conversions that allow for the creation of (DSLs).\footnotemark[\value{footnote}]

\section{Growing a language}

Scala's notion of "growing a language" distinguishes it from programming languages. It involves the ability to expand and customize the language to fit a field or issue. This principle is deeply ingrained in Scala's design philosophy, which strives to offer an expressive groundwork that developers and library creators can expand upon.\footnotemark[\value{footnote}]

At the heart of evolving a language, in Scala, lies the notion of \textbf{domain-specific languages (DSLs)}. These are mini languages or APIs designed for a specific domain or problem. They present an expressive and intuitive approach to tackling problems within that domain. Scala's syntax, \textbf{type system} and abstraction mechanisms make it exceptionally well suited for crafting DSLs. Features  — like \textbf{operator overloading}, \textbf{implicit conversions}, and \textbf{higher-order functions} — empower developers to craft allow developers to create APIs that feel like natural extensions of the language itself.\footnotemark[\value{footnote}]

A crucial factor enabling the evolution of a language in Scala is its abstraction capabilities. Scala offers an array of tools, for abstracting patterns and constructing combinable components. For instance, \textbf{traits} allow developers to define reusable interfaces and implementations that can be incorporated into classes as required. \textbf{Higher-order functions} and type classes make it possible to create abstractions that can be used across multiple data types and structures. These abstraction mechanisms are crucial, for developing libraries and frameworks that can grow the language in new and innovative ways.\footnotemark[\value{footnote}]

Another key aspect of Scala's language growth is its focus on community-driven development. Scala boasts an ecosystem of libraries and frameworks that enhance the language in ways. These libraries are often created by developers who have encountered a particular problem or need in their own work and have used Scala's language-growing capabilities to create a solution. This collaborative approach ensures that Scala stays relevant and adaptable to the changing needs of developers and the software industry as a whole.\footnotemark[\value{footnote}] \footnotetext{\fullcite{odersky.etal_2021}}

Scala allows extending and adapting the language seamlessly through the creation of custom libraries that feel like native language features. The unified type system in Scala plays a role in its extensibility, allowing user-defined types — like  \textbf{classes}, \textbf{traits} or \textbf{objects} — to stand on par with built-in types. This means libraries can define new types that feel as natural and integrated as Int, String, or List.\footnotemark{}

Advanced metaprogramming features such, as Scala's \textbf{macros} and \textbf{DSLs}, take language extension to the next level. \textbf{Macros} allow libraries to generate code at compile-time based on user-supplied expressions, supporting code generation and optimizations. \textbf{Domain-specific languages (DSLs)} enable libraries to establish syntax that aligns with their area of focus, enhancing the clarity and intent of user written code.\footnotemark[\value{footnote}] \footnotetext{\fullcite{ghoshDSLsAction2011}}

\section{What makes Scala scalable?}

Firstly, Scala's smooth integration of object-oriented and functional programming plays a fundamental role, in enabling scalability. The object-oriented elements, provide the structure and modularity necessary for building large, complex systems. They facilitate the development of reusable, encapsulated components that can be combined and expanded as the codebase expands. On the other hand, the functional aspects, support scalability. By blending these two paradigms, developers can write code that's both modular and scalable to meet the evolving needs of the system.\footnote[1]{\fullcite{odersky.etal_2021}}

Secondly, Scala's expressive static type system is a key factor in its scalability. The \textbf{type system} serves as a tool for catching bugs at compile-time, reducing the chances of runtime errors and increasing code reliability. It supports scalable code evolution through various features that enable the creation of flexible and reusable abstractions. These abstractions can adapt to changing requirements without sacrificing type safety or requiring extensive code rewrites. Moreover, the type system encourages developers to express their intent clearly, making the codebase more maintainable and easier to reason about as it grows in size and complexity.\footnotemark[1]

Lastly, Scala's high-level abstractions for data processing and concurrency are essential for writing scalable code. Abstractions — like map, flatMap, and fold (explained later) — allow developers to express complex data transformations and computations in a concise and declarative manner. These abstractions are inherently scalable, as they can be efficiently parallelized and distributed across multiple cores or machines, enabling the processing of large datasets with ease.\footnotemark[1]

In terms of concurrency, Scala provides powerful constructs like Futures and Promises, which simplify the creation of asynchronous and non-blocking code. These abstractions, along with frameworks like Akka these features, empower developers to construct responsive and resilient systems without needing to handle low-level concurrency primitives directly.\footnotemark[1]

\section{Object-oriented paradigm in Scala}
Scala is a multi-paradigm language that seamlessly combines object-oriented and functional programming paradigms. It builds upon and extends the object-oriented paradigm found in Java, while also providing functional programming capabilities.\footnotemark \footnotetext{\fullcite{joshuad.suerethScalaDepth2012}}

One of the key benefits of object-oriented programming is modularity and encapsulation. \textbf{Classes} and \textbf{objects} allow encapsulating related data and behavior into logical units, promoting code organization and reducing complexity. By encapsulating data processing logic within \textbf{classes} and \textbf{objects}, self-contained and reusable components can be created and easily combined to build data processing pipelines. This modular approach enhances code maintainability and testability, which are crucial for large-scale data engineering projects.\footnotemark[3]

In addition, Scala supports \textbf{single inheritance} through classes, allowing for the creation of hierarchical relationships between data structures and processing logic. Common behavior and attributes can be defined in base classes and specialized in derived classes, promoting code reuse and reducing duplication.\footnotemark[3]

\textbf{Traits} provide a flexible mechanism for composing behavior and defining common interfaces. They can be mixed into classes to add functionality horizontally, enabling the creation of modular and reusable data processing components. \textbf{Inheritance} and \textbf{trait composition} allow defining common data processing operations, such as data transformations, aggregations, and persistence, which can be shared across different stages of the data pipeline.\footnotemark[3]

In Scala, \textbf{every value is an object, and every operation is a method call.} This uniform object model is a key aspect of Scala's object-oriented nature.\footnotemark[3]
\input{tables/table_1}

\section{Overview of functional programming paradigm}

Functional programming (FP) is a programming paradigm based on the principles of lambda calculus, where programs are constructed by applying and composing functions.  In FP, computation is treated as the evaluation of mathematical functions, avoiding changing state and mutable data.\footnotemark \footnotetext{\fullcite{StenbergFunctionalAI}}

Functional programming has its roots in lambda calculus, a formal system developed by Alonzo Church in the 1930s to investigate computability. The first functional programming language, Lisp, was developed by John McCarthy in 1958. Other early influential functional languages include: APL (1966), ML (1973), Scheme (1975), Miranda (1985), Haskell (1990). In recent years, functional programming has seen a resurgence in popularity, with modern multi-paradigm languages like Scala, F\#, Clojure, and Elixir supporting a functional style. Many mainstream languages like Java, C\#, and Python have also added functional features. \footnotemark \footnotetext{\fullcite{Kunasaikaran2016ABO}}

\section{Core features of functional programming}

This section provides an overview of key functional programming concepts utilized in Scala for data engineering. These concepts include immutable data structures, higher-order functions, lazy evaluation, pattern matching, algebraic data types, type classes, monads, error handling, and parallel and distributed processing. As the scope of this thesis does not encompass a comprehensive explanation of functional programming or Scala, each concept will be briefly introduced along with a concise practical code example and explanation. A solid grasp of these concepts is essential for constructing data pipelines, which will be further elaborated upon in subsequent chapters when relevant. By focusing on the core principles and their practical applications, this section lays the foundation for understanding how functional programming in Scala facilitates effective data engineering practices.

\subsection{Immutable data structures}

Immutable data structures are fundamental, in functional programming. It plays a key role, in constructing data pipelines. In Scala, these immutable data structures form a component of the language and its standard library. An immutable data structure is one that remains unchanged once created. Once its structure is set, it maintains a state throughout its existence. Any action that seems to alter the structure actually returns a new instance of the data structure with the intended modifications, leaving the original data structure unchanged. This characteristic of immutability offers advantages.\footnotemark \footnotetext{\fullcite{zibinObjectReferenceImmutability2007}}

\subsubsection{Data consistency}

To begin with, data consistency guarantees that the data stays consistent and predictable throughout the data processing pipeline. By not allowing modifications to the data, it eliminates any chances of alterations or unintended changes or side effects that could result in data corruption or inconsistencies. This aspect holds importance in situations involving distributed and concurrent processing, where various processes or threads might be interacting with the data concurrently.\footnotemark \footnotetext{\fullcite{milewskiFunctionalDataStructures2013}}

\subsubsection{Thread safety}

One more benefit is that data structures are naturally thread-safe, since they can't be changed by multiple threads concurrently. This removes the need, for synchronization mechanisms. Another thing, it lowers the risk of race conditions and other concurrency-related bugs. Data can be securely shared and accessed by multiple threads without using locks or other synchronization primitives, making it easier to create concurrent and parallel data processing pipelines.\footnotemark[7]

\subsubsection{Fault tolerance}

The third advantage is that, it allows for fault-tolerant data processing by providing a stable and reproducible state. Even if there are failures or exceptions, the data stays intact. The processing can restart from a recognized point. This feature is especially valuable, in distributed systems where failures are frequent and the capability to bounce back from setbacks is essential, for upholding data integrity and consistency.\footnote[7]{\fullcite{milewskiFunctionalDataStructures2013}}

\subsubsection{Practical examples}

Scala offers a rich set of immutable data structures in its standard library, including \textbf{List}, \textbf{Vector}, \textbf{Map} and \textbf{Set}. These data structures are designed for efficiency. Tailored to practical scenarios. For instance, \textbf{List} is a linked-list implementation that provides constant-time prepending and tail access, making it suitable for recursive algorithms and pattern matching. On the other hand, \textbf{Vector} is a tree-based data structure that provides efficient random access and updates, making it suitable for indexed data processing.\footnotemark \footnotetext{\fullcite{scalaVector}}

\input{tables/table_2}

Furthermore, immutable data can be easily shared between different parts of the data processing pipeline. Because the data remains unchanged, it can be safely shared and reused without the need for defensive copying or other synchronization mechanisms. This approach often results in performance enhancements, particularly when handling large datasets.\footnotemark \footnotetext{\fullcite{tomeDataEngineeringScala2024}}

\input{tables/table_3}

Immutable data structures also play a crucial role in enabling lazy evaluation and optimizing data processing pipelines. Lazy evaluation allows for the deferred computation of intermediate results until they are actually needed, reducing memory overhead and improving performance. Scala's data structures, such as Stream and Iterator, provide lazy evaluation capabilities out of the box, enabling the creation of efficient and memory-friendly data processing pipelines.\footnote[9]{\fullcite{tomeDataEngineeringScala2024}}

\input{tables/table_4}

\subsection{Higher-order functions}

Higher-order functions enable the creation of modular, reusable, and composable data processing pipelines, leading to more efficient and maintainable code. In Scala, higher-order functions are a core language feature and are extensively used in data transformation, aggregation, and analysis tasks.\footnotemark \footnotetext{\fullcite{michael.etal_2023}}

A higher-order function is a type of function that takes one or more functions as inputs, gives back a function as an output, or does both. This feature of treating functions as first-class citizens permits the development of adaptable data processing tasks. Such functions facilitate adjusting behavior parameters and combining functions together.\footnotemark[10]

Scala's standard library provides a rich set of higher-order functions that are commonly used in data engineering tasks. These functions operate on collections and enable concise and expressive data manipulation.\footnotemark[10]

\input{tables/table_5}

\input{tables/table_6}

\input{tables/table_7}

\input{tables/table_8}

Higher-order functions enable the creation of reusable and composable data processing operations. By parameterizing behavior through functions, it is possible to build data processing pipelines that can be easily adapted to different datasets and requirements.\footnotemark[10]

Higher-order functions also enable the creation of \textbf{domain-specific languages (DSLs)} and fluent \textbf{APIs for data processing}. By defining a set of HOFs that capture common data processing patterns and operations, data engineers can create expressive and readable code that closely resembles the problem domain.\footnote[10]{\fullcite{michael.etal_2023}}

\input{tables/table_9}

\input{tables/table_10}

\subsection{Lazy evaluation}

Lazy evaluation is a technique where the evaluation of an expression is delayed until its value is actually needed. It enables efficient processing of large datasets, improves performance by avoiding unnecessary computations, and allows for the creation of infinite data structures.\footnotemark \footnotetext{\fullcite{scalaLazy}}

In Scala, lazy evaluation is supported through the use of \textbf{lazy values}, \textbf{lazy collections}, and \textbf{lazy data processing operations}. Lazy values are defined using the lazy keyword and are evaluated only when they are accessed for the first time. Subsequent accesses to a lazy value reuse the previously computed result, avoiding redundant computations.\footnotemark[11]

\input{tables/table_11}

Lazy collections, such as \textbf{Stream} and \textbf{Iterator}, are another important aspect of lazy evaluation in Scala. These collections represent potentially infinite sequences of elements and provide a way to process data in a lazy and incremental manner. Elements of a lazy collection are computed on-demand, allowing for the efficient processing of large or infinite datasets.\footnotemark \footnotetext{\fullcite{hughesWhyFunctionalProgramming1990}}

\input{tables/table_12}

Lazy evaluation is particularly useful in scenarios where the processing of large datasets is involved. By leveraging lazy evaluation, one can create scalable data processing workflows capable of managing vast amounts of data measured in terabytes or petabytes. Lazy evaluation allows for the incremental processing of data, where only the necessary computations are performed, and intermediate results are generated on-demand.\footnotemark \footnotetext{\fullcite{chenE3ElasticExecution2011}}

Scala's functional programming libraries, such as \textbf{Apache Spark} and \textbf{Akka Streams}, heavily rely on lazy evaluation to enable distributed and parallel data processing. In Apache Spark, lazy evaluation is used to build a \textbf{directed acyclic graph (DAG)} of transformations and actions, which is optimized and executed in a distributed manner across a cluster of machines. Lazy evaluation allows \textbf{Spark} to optimize the execution plan, minimize data movement, and improve overall performance.\footnotemark[13]

\input{tables/table_13}

Lazy evaluation also enables the creation of modular and composable data processing pipelines. By defining data transformations and computations lazily, data engineers can build reusable and flexible components that can be easily combined and adapted to different data processing scenarios. This promotes code reuse, improves maintainability, and allows for the creation of complex data processing workflows.\footnotemark[13]

\input{tables/table_14}

\subsection{Pattern matching and algebraic data types (ADTs)}

Pattern matching and algebraic data types provide an expressive way to model, process, and retrieve information from complex data structures. Pattern matching allows for concise and readable code, while ADTs enable the creation of rich and type-safe data models.\footnotemark[14]

Algebraic data types are a way to define complex data structures using a combination of product types (tuples or case classes) and sum types (sealed traits or enums). Product types represent data that contains multiple fields, while sum types represent data that can be one of several possible variants. ADTs allow for the modeling of hierarchical and recursive data structures, making them particularly useful.\footnotemark[14]

In Scala, ADTs are typically defined using sealed traits and case classes. Sealed traits define a closed set of possible variants, ensuring that all cases are covered and preventing the addition of new variants outside the defined set. Case classes, on the other hand, represent the individual variants of the ADT and provide a convenient way to define and manipulate data.\footnote[14]{\fullcite{michael.etal_2023}}

\input{tables/table_15}

Pattern matching is a feature in Scala that allows for the concise and expressive processing of ADTs. It enables the deconstruction and extraction of data based on its structure and variant. Pattern matching provides a way to write concise and readable code that can handle different cases of an ADT elegantly.\footnotemark[14]

\input{tables/table_16}

Pattern matching can also be useful when extracting and rearranging data. It enables the targeted extraction of data using patterns and conditions. This proves beneficial when data requires filtering, transforming or aggregating based on certain criteria.\footnotemark[14]

\input{tables/table_17}

Pattern matching and ADTs are particularly useful in data engineering when working with complex and hierarchical data structures. They allow for the creation of expressive and type-safe data models that can be easily processed and transformed. Scala's support for pattern matching and ADTs, along with its integration with big data frameworks like Apache Spark.\footnote[14]{\fullcite{michael.etal_2023}}

\input{tables/table_18}

\subsection{Type classes}

In functional programming, type classes are an idea that allows for ad-hoc polymorphism and offers a way to define generic behavior for types without altering their primary definitions. These classes are crucial as they facilitate the development of reusable and composable abstractions that're applicable, to various data types and structures.\footnote[1]{\fullcite{odersky.etal_2021}}

In Scala, type classes are implemented using implicit parameters and implicit definitions. A type class is defined as a trait that declares a set of operations or behaviors that can be implemented for different types. These operations are defined as abstract methods within the trait. Types that want to belong to the type class provide implicit implementations of these methods.\footnotemark[1]

\input{tables/table_19}

Type classes enable the development of generic functions that can be used with any type possessing the type implicit instance of the required type class. This empowers the formation of reusable and composable abstractions that can be implemented on different data types without altering their initial definitions.\footnotemark[1]

\input{tables/table_20}

Type classes prove to be quite handy, in situations where there is a need to define common behaviors or operations for different data types. They enable the development of generic and reusable data processing abstractions that can be utilized across a wide range of data structures.\footnotemark[1]

\input{tables/table_21}

Type classes are also commonly used in Scala's functional programming libraries, such as Cats and Scalaz, which provide a wide range of type classes for various algebraic structures and data processing operations. These libraries leverage type classes to define generic and composable abstractions for data processing, such as \textbf{Functor}, \textbf{Monad}, and \textbf{Traverse}, which can be applied to different data types and structures.\footnote[1]{\fullcite{odersky.etal_2021}}

\subsection{Monads and error handling}

Monads and error handling are essential concepts in functional programming that play a crucial role in building robust and fault-tolerant data pipelines. In Scala, monads provide a powerful abstraction for composing and chaining data processing operations, while also enabling effective error handling and data validation.\footnotemark \footnotetext{\fullcite{wadler1992monads}}

At its core, a monad is a design pattern that allows for the composition of computations in a way that is both expressive and type-safe. Monads encapsulate a computational context and define a set of operations for working with values within that context. The two fundamental operations of a monad are \textbf{flatMap} (also known as \textbf{bind}) and \textbf{pure} (also known as \textbf{unit} or \textbf{return}). These operations satisfy certain laws, known as the monad laws, which ensure their consistent behavior and enable their composition.\footnotemark[14]

In Scala, monads are implemented using the \textbf{flatMap}, \textbf{map}, and \textbf{pure} methods. The \textbf{flatMap} method allows for the chaining of computations that may produce a monad instance, while the \textbf{map} method applies a function to the value inside the monad. The \textbf{pure} method lifts a value into the monad context. Scala's standard library provides monad implementations for common types such as \textbf{Option}, \textbf{Either}, \textbf{Try}, and \textbf{Future}, each serving different purposes in error handling and data processing.\footnote[3]{\fullcite{joshuad.suerethScalaDepth2012}}

One of the primary use cases of monads in data engineering is error handling and data validation. Monads like \textbf{Option} and \textbf{Either} provide a way to represent and propagate errors or absence of values through a data processing pipeline. \textbf{Option} is used to represent a value that may or may not be present, while \textbf{Either} represents a value that can be either a success or a failure. By using these monads,  it can explicitly handle and propagate errors, ensuring that the data pipeline remains in a consistent state and avoids unexpected failures.\footnotemark[3]

For example, considering a data processing pipeline that involves reading data from a file, parsing it, and performing some transformations. Each step in the pipeline can potentially fail, such as the file not being found, the data being in an invalid format, or the transformations resulting in an error. By using monads like \textbf{Either}, it is possible to capture and propagate these errors through the pipeline, allowing for centralized error handling and reporting.\footnotemark[3]

\input{tables/table_22}

By using monads for error handling, it is possible to build data pipelines that are more resilient, maintainable, and easier to reason about. Monads provide a consistent and composable way to handle errors, allowing for centralized error handling and reporting, and ensuring that the data pipeline remains in a predictable state.\footnotemark[3]

In addition to error handling, monads also play a crucial role in data validation and transformation. Monads like \textbf{Option} and \textbf{Either} can be used to represent and propagate missing or invalid data through the pipeline, allowing for explicit handling and reporting of data quality issues. Monads can also be used to encapsulate and compose data transformations, ensuring that the transformations are applied consistently and in a type-safe manner.\footnotemark[3]

Furthermore, monads are particularly useful in the context of parallel and distributed data processing. Monads like \textbf{Future} and \textbf{Task} (from the Monix library) provide abstractions for asynchronous and parallel computations, allowing for the composition and coordination of distributed data processing tasks. By using monads, it is possible to build data pipelines that can scale horizontally and handle large volumes of data efficiently.\footnote[3]{\fullcite{joshuad.suerethScalaDepth2012}}

\subsection{Parallel and distributed processing}

Parallel and distributed processing are crucial aspects of data engineering that enable the efficient processing of large volumes of data. Scala, with its libraries and frameworks, provides an excellent foundation for building scalable and fault-tolerant parallel and distributed data processing pipelines. \textbf{Immutable data structures} ensure that data can be safely shared and processed concurrently without the risk of race conditions or inconsistencies. \textbf{Pure functions}, which always produce the same output for the same input, are inherently parallelizable and can be safely executed independently on multiple nodes in a distributed system. \textbf{Referential transparency} allows for the easy composition and reasoning about parallel and distributed computations.\footnote[9]{\fullcite{tomeDataEngineeringScala2024}}

Scala's standard library provides a range of tools for parallel processing, such as the Parallel Collections framework, which enables the parallel execution of operations on collections. \textbf{Parallel collections} can be used to parallelize data processing tasks, such as mapping, filtering, and aggregation, across multiple CPU cores. This can significantly improve the performance of data processing pipelines, especially when dealing with large datasets.\footnotemark[9]

\input{tables/table_23}

While parallel processing can improve performance on a single machine, distributed processing is necessary when dealing with massive datasets that exceed the capacity of a single node. Scala has a rich ecosystem of libraries and frameworks that facilitate distributed data processing, such as Apache Spark and Akka.\footnotemark[9]

Apache Spark is a widely-used distributed computing framework that provides high-level APIs for processing large datasets across clusters of machines. Spark's Resilient Distributed Datasets (RDDs) and DataFrames allow for the distributed storage and processing of structured and unstructured data. Spark's functional programming APIs, such as \textbf{map}, \textbf{flatMap}, \textbf{filter}, and \textbf{reduce}, enable the concise and expressive composition of distributed data processing operations.\footnotemark[9]

\input{tables/table_24}

Akka is another powerful framework for building distributed systems in Scala. Akka's actor model provides a high-level abstraction for concurrent and distributed computing, allowing for the creation of scalable and fault-tolerant systems. Actors are lightweight, independent entities that communicate through asynchronous message passing, enabling the distribution of data processing tasks across a cluster of nodes.\footnote[9]{\fullcite{tomeDataEngineeringScala2024}}

\input{tables/table_25}