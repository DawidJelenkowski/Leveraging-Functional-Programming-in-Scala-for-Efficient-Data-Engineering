\chapter{Core Principles of Functional Programming in Scala for Data Engineering}
%\label{chap:functional_programing}
%\addcontentsline{toc}{chapter}{\numberline{\thechapter}The Functional programing Paradigm}

In recent years, the field of information technology has experienced exponential growth. To address the demands of managing and analyzing ever-growing data. Certain solutions were created. As organizations grapple with the challenges of big data, they require data processing systems that can be scaled, efficient, and easy to maintain. Functional programming has emerged as a potent strategy for constructing such systems, delivering advantages like immutability, modularity, and parallelism that align with data-driven demands.

The chapter explores how Scala, a multi-paradigm language that seamlessly blends object-oriented and functional programming, can be leveraged to create robust data engineering solutions. The core principles of functional programming in Scala and how they are applied specifically to data engineering tasks were examined.

The chapter begins by discussing the history and design philosophy of Scala, highlighting its ability to "grow" as a language through the incorporation of domain-specific languages. Afterward, acquire knowledge regarding the critical components that render Scala an effective tool for data engineering, including its compatibility with both functional and object-oriented approaches.

    \begin{enumerate}
        \item \textbf{Immutable data structures (IDS)} and their benefits for data consistency and concurrency;
        \item \textbf{Higher-order functions (HOFs)} for building flexible and reusable data processing pipelines;
        \item \textbf{Lazy evaluation (LE)} for efficient handling of large datasets;
        \item \textbf{Pattern matching (PM)} and \textbf{algebraic data types (ADTs)} for expressive data modeling and transformation;
        \item \textbf{Type classes (TC)} for creating generic, extensible abstractions;
        \item \textbf{Monads (M)} and functional error handling for robust data pipelines;
        \item \textbf{Parallel and distributed processing (PDP)} capabilities.
    \end{enumerate}

Throughout the chapter, code examples were provided to illustrate how these concepts can be implemented in practical data engineering scenarios.

\section{Background}

Scala stands for a \textbf{“scalable language”}. Scala was created by German computer scientist \textbf{Martin Odersky}. Odersky worked on the Java compiler and generics development at Sun Microsystems. He wanted to make a language that combines both object-oriented and functional programming styles. He wanted to make a typed language that works with the \textbf{Java Virtual Machine (JVM)}. The very first version of Scala appeared in 2003, marking the beginning of its journey. Over time, it has become a widely used language with a plethora of libraries and tools (\cite{oderskyOverviewScalaProgramming2006})\footnote[1]{\fullcite{oderskyOverviewScalaProgramming2006}}.

Scala is a language that combines ideas from \textbf{object-oriented and functional programming}. The design is inspired by several other languages. Scala uses \textbf{Java's syntax and object-oriented features}, so it's easy for Java developers to learn and works well with Java code. Scala has many things in common with Haskell and ML, like having immutable data structures, higher-order functions and pattern matching. Scala uses the ML family of languages for its type system. It has a static type system with a type interface. Scala also tries to be as short and simple as scripting languages like Python. It has features like operator overloading and implicit conversions that make DSLs (\cite{odersky.etal_2021})\footnote[2]{\fullcite{odersky.etal_2021}}.

\section{Growing a Language}

Scala's was designed with the idea of \textbf{"growing a language"}, which sets it apart from other programming languages. The language was designed to give the user only the core functionalities and the ability to build libraries upon them. Meaning, user on its own designs a new language based on Scala for solving the particular problem or issue (\cite{odersky.etal_2021})\footnotemark[2].

This concept is called \textbf{domain-specific languages} or \textbf{DSLs}, for short. In practice, these are \textbf{mini-languages} or application programming interfaces \textbf{(APIs)}. These languages can be built in an expressive and intuitive manner for the end user. Meaning that they can be read and understood by people who have nothing to do with programming but are domain experts. Scala's syntax, type system and abstraction mechanisms make the tool well suited for crafting DSLs. Features — like operator overloading, implicit conversions and higher-order functions — empower developers to craft allow developers to create APIs that feel like natural extensions of the language itself (\cite{odersky.etal_2021})\footnotemark[2].

Key factors that help a Scala evolve are its \textbf{abstraction abilities}. Scala can be used to make patterns and components that can be combined. \textbf{Traits}, for instance, allow programmers to define repeatable interfaces and implementations that can be incorporated into classes as needed. \textbf{Higher-order functions} and \textbf{type classes} help create abstractions that can be used with many types of data. These abstraction mechanisms hold significant importance in the development of libraries and frameworks that can augment the language in novel ways (\cite{odersky.etal_2021})\footnotemark[2].

Having that said, the idea of \textbf{DSLs} wouldn't make any sense without \textbf{community-driven development}. Scala contains a rich ecosystem of libraries and frameworks, of which a few are discussed in the later chapters. These libraries are often made by people who have encountered a problem and decided to use Scala to solve it. Thanks to this design, Scala can stay relevant and adapt to the changing needs of developers or the industry in general (\cite{odersky.etal_2021})\footnotemark[2].

The design of Scala ensures that \textbf{custom libraries feel like native language features}. The unified type system in Scala makes it easy to add new types — like classes, traits or objects — that can be used with other built-in types in pairs. Meaning, libraries can create new types that are as natural as Integers, Strings or Lists (\cite{ghoshDSLsAction2011})\footnotemark[3].

\section{What Makes Scala Scalable?}

Scala's smooth \textbf{integration between object-oriented and functional programming} is important for scalability. Object-oriented elements give the structure and flexibility needed for making big and complicated systems. They allow developers to create components that can be combined and expanded as the codebase grows. On the other hand, the functional aspects, support scalability. By combining these two approaches, developers can write code that's both modular and flexible to meet the changing needs of the system (\cite{odersky.etal_2021})\footnote[2]{\fullcite{odersky.etal_2021}}.

In addition, Scala's ability to use static types is significant for making it \textbf{easy to scale}. The \textbf{type system} helps find errors early, reduces errors during runtime and makes code more reliable. It supports scalable code evolution through various features that enable the creation of flexible and reusable abstractions. These abstractions can adapt to changing requirements without sacrificing type safety or requiring extensive code rewrites. It is easier to reason about as the codebase grows in size and complexity because the type system encourages developers to express their intent clearly (\cite{odersky.etal_2021})\footnotemark[2].

Lastly, Scala's high-level abstractions for data processing and concurrency are the best feature for writing scalable code. \textbf{Abstractions} — like \textbf{map}, \textbf{flatMap} and \textbf{fold} (explained later) — allow developers to express complex data transformations and computations in a concise and declarative manner. These abstractions can be used on multiple cores or machines, which makes it easy to process large datasets (\cite{odersky.etal_2021})\footnotemark[2].

In terms of \textbf{concurrency}, Scala has useful tools known as \textbf{Futures} and \textbf{Promises} that make it easier to create \textbf{asynchronous} and \textbf{non-blocking code}. Using abstractions and tools like \textbf{Akka} help developers to create \textbf{responsive} and \textbf{resilient systems} that can endure without the  need to handle low-level concurrency primitives directly (\cite{odersky.etal_2021})\footnote[2]{\fullcite{odersky.etal_2021}}.

\section{Object-Oriented Paradigm in Scala}

As previously stated, \textbf{Scala is based on the JVM}. Consequently, the code is written in a manner that is similar to that of Java. Object-oriented programming, together with Scala’s functional programing qualifies Scala as a \textbf{multi-paradigm language.} In short, as a result, \textbf{every value is an object and every operation is a method call} (\cite{joshuad.suerethScalaDepth2012})\footnote[4]{\fullcite{joshuad.suerethScalaDepth2012}}.

Object-oriented programming offers excellent \textbf{modularity} and \textbf{encapsulation}, resulting in \textbf{reusable code}. \textbf{Classes} and \textbf{objects} help organize data and behaviors into logical parts. With this approach, code becomes significantly more streamlined and organized. By putting a data into a processing logic in classes and objects, components can be made and used to build \textbf{data processing pipelines}. This modular approach makes code easier to maintain and test, which holds great significance in big data projects (\cite{ghoshDSLsAction2011})\footnotemark[3].

In addition, Scala supports \textbf{single inheritance through classes}, which allows for hierarchical relationships between data structures and processing logic. Common behaviors and attributes can be defined in base classes and specialized in derived classes, which helps eliminate redundant code and promotes code reuse (\cite{ghoshDSLsAction2011})\footnotemark[3].

\textbf{Traits} are a flexible way to combine behavior and define common interfaces. Adding functionality horizontally by mixing it into classes allows for the creation of modular and reusable data processing elements. Common data processing operations — such as \textbf{data transformation, aggregation and persistence} — can be defined across different stages of the data pipeline based on inheritance and trait composition (\cite{ghoshDSLsAction2011})\footnote[3]{\fullcite{ghoshDSLsAction2011}}.

\input{tables/table_1}

\section{Overview of Functional Programming Paradigm}

\textbf{Functional programming} (\textbf{FP}) is a way of making programs that uses lambda calculus. Programs are created by combining functions together. In \textbf{FP}, computation means evaluating mathematical functions without changing their state or modifying data (\cite{StenbergFunctionalAI})\footnote[5]{\fullcite{StenbergFunctionalAI}}.

Functional programming is based on lambda calculus, a formal system developed by Alonzo Church in the 1930s to study computerability. The first functional programing language, Lisp, was developed by John McCarthy in 1958. Other early influential functional languages include: APL (1966), ML (1973), Scheme (1975), Miranda (1985), Haskell (1990). In the last few years, functional programming has become more popular. Modern multi-paradigm languages like Scala, F\#, Clojure and Elixir all support functional programming. Many mainstream languages like Java, C\# and Python have also added functional features (\cite{Kunasaikaran2016ABO})\footnote[6]{\fullcite{Kunasaikaran2016ABO}}.

\section{Core Features of Functional Programing}

The following section provides an overview of \textbf{key functional programming concepts} utilized in Scala \textbf{for data engineering}. These concepts include immutable data structures, higher-order functions, lazy evaluation, pattern matching, algebraic data types, type classes, monads, error handling and parallel and distributed processing. This thesis does not attempt to provide a comprehensive explanation of functional programming or Scala. Instead, it provides a concise explanation of each concept and offers a straightforward example to demonstrate its usage. Having a solid grasp of these concepts is crucial for building efficient data pipelines. This section focuses on the core principles and how they can be used in data engineering practices.

\subsection{Immutable data structures}

\textbf{Immutable data structures} are fundamental, in functional programing. It plays a key role, in constructing data pipelines. In Scala, these types of data structures are part of the language and its standard library. An immutable data structure is one that \textbf{stays the same after it is created}. \textbf{Once its structure is set, it stays that way all the time}. The original data structure is left unchanged by any action that seems to alter the structure (\cite{zibinObjectReferenceImmutability2007})\footnote[7]{\fullcite{zibinObjectReferenceImmutability2007}}.

As the name "immutable" indicates, \textbf{data consistency} makes sure that the data stays the same throughout the data processing pipeline. By not letting changes to the data happen, there is no chance for the data to get corrupted or inconsistent. This is crucial when many processes or threads are working on the data at once (\cite{milewskiFunctionalDataStructures2013})\footnotemark[8].

Another advantage is that data structures are inherently \textbf{thread-safe}, as they cannot be modified by multiple threads simultaneously. This eliminates the necessity for synchronization mechanisms. In addition, it helps to reduce the likelihood of race conditions and other bugs related to concurrency. Data can be easily shared and accessed by multiple threads without the need for locks or other synchronization primitives (\cite{milewskiFunctionalDataStructures2013})\footnotemark[8].

Another notable benefit is \textbf{fault tolerance}. It is the ability to handle errors and continue functioning effectively. Regardless of any circumstances, the data remains consistent. The processing can resume from a recognized point. This feature is incredibly valuable for systems that experience frequent failures and require the ability to recover from setbacks in order to maintain accurate and consistent data (\cite{milewskiFunctionalDataStructures2013})\footnote[8]{\fullcite{milewskiFunctionalDataStructures2013}}.

Scala has many types of immutable data — like \textbf{List}, \textbf{Vector}, \textbf{Map} and \textbf{Set} — in its standard library. These\textbf{ data structures} (\textbf{DS}) were created to maximize efficiency. Tailored to practical scenarios. For example, List is a \textbf{linked-list} implementation that can be used for recursive algorithms and pattern matching. On the other hand, the \textbf{Vector} \textbf{DS} is based on trees, which provide impressive random access and updates, making it perfect for indexed data processing  (\cite{scalaVector})\footnote[9]{\fullcite{scalaVector}}.

\input{tables/table_2}

In addition, when data cannot be changed, it allows for easy sharing  between different parts of the data processing pipeline. Since the information stays the same, it can be shared and used again without worrying about copying or synchronizing. It especially shines when dealing with large datasets by improving performance (\cite{tomeDataEngineeringScala2024})\footnote[10]{\fullcite{tomeDataEngineeringScala2024}}.

\input{tables/table_3}

The fact that data is immutable, results in the ability to create and use \textbf{lazy evaluation} (\textbf{LE}) and optimize data processing pipelines. \textbf{LE} lets delay the computation of intermediate results until they are needed, which results in reduced memory overhead and improved overall performance. Scala's data structures — such as \textbf{Stream} and \textbf{Iterator} — provide \textbf{LE} capabilities out of the box, which makes it easy to create memory-friendly data pipelines (\cite{tomeDataEngineeringScala2024})\footnote[10]{\fullcite{tomeDataEngineeringScala2024}}.

\input{tables/table_4}

\subsection{Higher-Order Functions}

\textbf{Higher-order functions} (\textbf{HOFs}) are yet another killer feature; basically, they help make data processing systems that can be easily changed and reused. Through their utilization, the code can be fine-tuned to be faster and easier to maintain. These functions typically used for data transformation, grouping and analysis (\cite{michael.etal_2023})\footnote[11]{\fullcite{michael.etal_2023}}.

The \textbf{HOF} can be defined as a type of function that takes one or more functions as inputs, gives back a function as an output, or does both. In more complex terms, this feature of treating \textbf{functions as first-class citizens} permits the development of adaptable data processing tasks. These functions help change behavioral parameters and combine functions. (\cite{michael.etal_2023})\footnotemark[11].

In the case of examples, the Scala standard library provides a rich set of tools that are commonly used in data engineering tasks. These functions operate on collections and enable concise and expressive data manipulation. Below are the most popular ones (\cite{michael.etal_2023})\footnotemark[11].

\input{tables/table_5}

\input{tables/table_6}

\input{tables/table_7}

\input{tables/table_8}

As shown in the 1.9 table, \textbf{HOFs} enable data processing operations that can be reused regardless of the data type. With the ability to establish the internal structure of systems using functions, one can create data processing pipelines that are adaptable to various data types and requirements (\cite{michael.etal_2023})\footnotemark[11].

\textbf{HOFs} also enable the creation of \textbf{domain-specific languages} (\textbf{DSLs}) and fluent \textbf{APIs} for data processing. By defining a set of \textbf{HOFs}, data engineers can make code that looks like the problem domain (\cite{michael.etal_2023})\footnote[11]{\fullcite{michael.etal_2023}}.

\input{tables/table_9}

\input{tables/table_10}

\subsection{Lazy Evaluation}

\textbf{Lazy evaluation} \textbf{(LE)} is a technique where the evaluation of an expression is delayed until its value is actually needed  — look at table 1.11. It makes big amounts of information work faster, avoids unnecessary work and let make endless data structures (\cite{scalaLazy})\footnote[12]{\fullcite{scalaLazy}}.

In Scala, \textbf{LE} is possible by using \textbf{Lazy Values} (\textbf{LV}), \textbf{Lazy Collections} (\textbf{LC}) and \textbf{lazy data processing}. In terms of \textbf{LV}, they  are only evaluated when they are accessed for the first time. Later accesses to a \textbf{LV} reuse the result that was previously computed (\cite{michael.etal_2023})\footnote[11]{\fullcite{michael.etal_2023}}.

\input{tables/table_11}

In the case of \textbf{LC}, — such as \textbf{Stream} and \textbf{Iterator} — a lazy and incremental approach to processing data is provided by these collections, which represent potentially infinite sequences of elements. A \textbf{LCs} elements are computed on demand, which makes it easy to process large or infinite datasets (\cite{hughesWhyFunctionalProgramming1990})\footnote[13]{\fullcite{hughesWhyFunctionalProgramming1990}}.

\input{tables/table_12}

A further perk of \textbf{LE} is its applicability in building massive data processing systems that can manage petabytes or terabytes of data. Additionally, \textbf{LE} can be implemented to incrementally process data, with an emphasis on the most critical computations. Furthermore, \textbf{LE} can be employed to process data incrementally, focusing solely on the essential computations (\cite{chenE3ElasticExecution2011})\footnote[14]{\fullcite{chenE3ElasticExecution2011}}.

In the case of Scala's functional programing libraries, — such as \textbf{Apache Spark} and \textbf{Akka Stream}; table 1.12 — they use \textbf{LE} a lot to process data in many places at once. In \textbf{Apache Spark}, \textbf{LE} is used to build a \textbf{directed acyclic graph} (\textbf{DAG}) — as can be seen in the 1.13 table — of transformations and actions that are optimized and executed in a distributed way across a cluster of machines. \textbf{Spark} can use \textbf{LE} to make the execution plan work better, move data less often and improve performance overall (\cite{michael.etal_2023})\footnote[11]{\fullcite{michael.etal_2023}}.

\input{tables/table_13}

By utilizing lazily defined data transformations and computations, data engineers can construct reusable and adaptable components that can be easily combined and adapted to diverse data processing scenarios — look at table 1.14. This strategy encourages code reuse, improves upkeep and aids in the development of intricate data processing workflows (\cite{michael.etal_2023})\footnote[14]{\fullcite{michael.etal_2023}}.

\input{tables/table_14}

\subsection{Pattern Matching and Algebraic Data Types (ADTs)}

\textbf{Pattern matching} and \textbf{algebraic data types} — for shorts,  \textbf{PM} and \textbf{ADTs} — provide an expressive way to model, process and retrieve information from complex data structures. \textbf{PM} makes code clear and easy to understand, and \textbf{ADTs} make data structures that are expressive and type-safe — look at table 1.15 (\cite{michael.etal_2023})\footnotemark[11].

Despite the complex name, \textbf{ADTs} are just a way to define complex data structures using a combination of product types — such as \textbf{tuples} or \textbf{case classes} — and summarize types — like \textbf{sealed traits} or \textbf{enums} —. Product types are data with multiple fields, while summarize types are data with only one field. What makes \textbf{ADTs} useful is that they allow for the modeling of hierarchical and recursive data structures (\cite{michael.etal_2023})\footnotemark[11].

They are typically defined using \textbf{sealed traits} and \textbf{case classes}. The first ones define a set of possible variants that covers all cases and prevents the addition of new variants outside the set. Case classes, on the other hand, represent the individual variants of the \textbf{ADT} and provide a convenient way to define and manipulate data (\cite{michael.etal_2023})\footnote[11]{\fullcite{michael.etal_2023}}.

\input{tables/table_15}

As mentioned before, the \textbf{PM} is a feature in Scala that allows for the concise and expressive processing of \textbf{ADTs} — look at table 1.16 —. Basically, it enables the deconstruction and extraction of data based on its structure and variant (\cite{michael.etal_2023})\footnotemark[11].

\input{tables/table_16}

\textbf{PM} can also be used to extract and reorganize data. It is done by using patterns and conditions to get data that needs to be filtered, transformed or grouped based on certain criteria  — look at table 1.17 (\cite{michael.etal_2023})\footnotemark[11].

\input{tables/table_17}

For example, when a data engineer has to work with complex and hierarchical data structures. He can create type-safe data models that can be easily processed and transformed — look at table 1.18 (\cite{michael.etal_2023})\footnote[11]{\fullcite{michael.etal_2023}}.

\input{tables/table_18}

\subsection{Type Classes}

In functional programing, \textbf{type classes} — for short, \textbf{TC} — are an idea that allows for ad-hoc polymorphism and offers a way to define generic behavior for types without altering their primary definitions. These classes are crucial as they facilitate the development of reusable and composable abstractions that're applicable, to various data types and structures (\cite{odersky.etal_2021})\footnote[2]{\fullcite{odersky.etal_2021}}. \textbf{TCs} are implemented using implicit parameters and implicit definitions — look at table 1.19 —; a \textbf{TC} is defined as a trait that declares a set of operations or behaviors that can be implemented for different types. These operations are defined as abstract methods within the trait. Types that want to belong to the \textbf{TCs} provide implicit implementations of these methods (\cite{odersky.etal_2021})\footnotemark[2].

\input{tables/table_19}

TCs help create generic functions — look at table 1.20 —; These are the functions that can be used with any type that has an implicit instance of the \textbf{TC} needed. In other words, it makes it possible to make abstractions that can be used on different data types without changing their original definitions (\cite{odersky.etal_2021})\footnotemark[2].

\input{tables/table_20}

TCs are handy when there is a need to define common behaviors or operations for different data types — despite lots of code table 1.21 explains it pretty well —; They can be used to make data processing abstractions that can be used in a wide range of data structures (\cite{odersky.etal_2021})\footnotemark[2].

\input{tables/table_21}

In terms of Scala libraries and \textbf{TCs} the Cats and Scalaz are the \textbf{TC} are also commonly used in Scala's functional programing libraries, such as Cats and Scalaz, which provide a wide range of type classes for various algebraic structures and data processing operations. These libraries leverage type classes to define generic and composable abstractions for data processing — such as Functor, Monad and Traverse — which can be applied to different data types and structures (\cite{odersky.etal_2021})\footnote[2]{\fullcite{odersky.etal_2021}}.

\subsection{Monads and error handling}

One of the hardest to understand of common concepts in Functional programming are \textbf{Monads} (\textbf{M}). In essence, they can be used to make and chain data processing operations, handle errors or validate data. Basically, a \textbf{Monad} is a design pattern that allows for the composition of computations in a way that is both expressive and type-safe. In simpler terms, it is a structure that wraps a value (or computation) and provides a standardized way to chain operations on that value
 (\cite{wadler1992monads})\footnote[15]{\fullcite{wadler1992monads}}.

The two fundamental operations are \textbf{flatMap} — also known as \textbf{bind} — and \textbf{pure} — also known as \textbf{unit} or \textbf{return} —. These operations follow certain rules called the monadic laws; basically, they make sure that they behave the same way (\cite{wadler1992monads})\footnote[15]{\fullcite{wadler1992monads}}.

In Scala, \textbf{Monads} are implemented using the \textbf{flatMap}, \textbf{map} and other \textbf{pure methods}. \textbf{A function can be called pure when it produces the same output for a given input, no matter what}. In this context, the flatMap binds computations that may produce a \textbf{Monad} instance, while the map method applies a function to the value inside the \textbf{M}. A value is inserted into the \textbf{M} context by the pure approach (\cite{joshuad.suerethScalaDepth2012})\footnote[4]{\fullcite{joshuad.suerethScalaDepth2012}}.

In the Scala's standard library there are \textbf{Monad} implementations for the most common types — such as \textbf{Option}, \textbf{Either}, \textbf{Try} and \textbf{Future} —, each serving different purposes in error handling and data processing. The most popular scenario for using \textbf{Ms} is data validation and error handling. \textbf{Ms} — like \textbf{Option} and \textbf{Either} — can be used to represent errors or missing values in a data pipeline. \textbf{Option} is used to represent a value that may or may not be present, while \textbf{Either} represents a value that can be either a success or a failure. In the result, the data pipeline remains in a consistent state and avoids unexpected failures (\cite{joshuad.suerethScalaDepth2012})\footnotemark[4].

For example, let's think about a data pipeline that involves a typical ETL scenario. Each step in the process can go wrong — like there is a missing file, record or a bug in transformation —; To solve these problems, \textbf{Either} can be used for centralized error handling and reporting by capturing these errors through the pipeline (\cite{joshuad.suerethScalaDepth2012})\footnotemark[4].

\input{tables/table_22}

\subsection{Parallel and Distributed Processing}

\textbf{Parallel and distributed processing} — \textbf{PDP} — in essence, responsible for the efficient processing of large volumes of data. Scala, with its libraries and frameworks, is the killer language for \textbf{PDP}. Immutable data structures make it safe to share and process data without the risk of race conditions or inconsistencies. Pure functions are inherently parallelizable and can be safely executed independently on multiple nodes in a distributed system (\cite{tomeDataEngineeringScala2024})\footnote[10]{\fullcite{tomeDataEngineeringScala2024}}.

The Parallel Collections library provides a base for parallel processing in Scala. It enables the parallel execution of operations on collections. They can be used to parallelize data processing tasks — such as mapping, filtering and aggregation, table 1.23 — across multiple CPU cores. In result, higher performance, especially on big data, is achieved. Parallel processing can accelerate a core work, but it's better to use more than one core to process a high volume of data (\cite{tomeDataEngineeringScala2024})\footnotemark[10].

\input{tables/table_23}

Scala has a rich ecosystem of libraries and frameworks that facilitate distributed data processing, such as Apache Spark and Akka. The first one provides high-level APIs for processing large datasets across clusters of machines. Spark's \textbf{Resilient Distributed Datasets}  \textbf{(RDD)} and \textbf{DataFrames} \textbf{(DF)} allow for the distributed storage and processing of structured and unstructured data — table 1.24 (\cite{tomeDataEngineeringScala2024})\footnotemark[10].

\input{tables/table_24}

The second framework is \textbf{Akka}. It has an actor model that provides a high-level abstraction for \textbf{concurrent and distributed computing}, which means that a \textbf{scalable} and \textbf{fault-tolerant} system can be made of them. Basically, actors are lightweight, independent entities that communicate through \textbf{asynchronous message passing}, enabling the distribution of data processing tasks across a cluster of nodes (\cite{tomeDataEngineeringScala2024})\footnotemark[10].

\input{tables/table_25}